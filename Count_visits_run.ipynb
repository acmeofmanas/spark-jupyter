{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilizing valid kerboros token to interact with cloudera \n",
    "# Run this cell when kerberos auth is expired\n",
    "\n",
    "!echo \"User :$USER\"\n",
    "yourPwd = input(\"Enter your password :\")\n",
    "\n",
    "!kdestroy\n",
    "!echo \"$yourPwd\" | kinit\n",
    "!klist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# Initilize find spark with valid spark home\n",
    "findspark.init('/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark', edit_rc=True)\n",
    "\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"expand_frame_repr\",True)\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datetime import date, timedelta, datetime \n",
    "import time \n",
    "import pyspark # only run this after findspark.init() \n",
    "from pyspark.sql import SparkSession, SQLContext \n",
    "from pyspark.context import SparkContext \n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * \n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName('standard placeiq queries'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_id = spark.sql(\"SELECT * FROM placeiq_analytics.category_id_count_per_chain_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_id.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_id_desc = spark.sql(\"SELECT * FROM placeiq_analytics.category_id_count_per_chain_new WHERE chain_id = 'lowes-home-improvement' ORDER BY dt DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_id_desc.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_20_place_rows = spark.sql(\"select * FROM placeiq_db_new.place LIMIT 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_20_place_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#top_20_place_rows.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_20_visit_rows = spark.sql(\"select * FROM placeiq_db_new.visit LIMIT 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_20_visit_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = config_dict['run_name'][0]\n",
    "# chain_ids = config_dict['chain_ids']\n",
    "# descriptions = config_dict['descriptions']\n",
    "# date_range = config_dict['date_range']\n",
    "# timeslice = config_dict['timeslice'][0]\n",
    "# dow_start = config_dict['dow_start']\n",
    "# states = config_dict['states']\n",
    "# dmas = config_dict['dmas']\n",
    "# chain_desc_operator = config_dict['chain_desc_operator'][0]\n",
    "# state_dma_operator = config_dict['state_dma_operator'][0]\n",
    "# usage_case = config_dict['usage_case'][0]\n",
    "\n",
    "run_name = 'lowes_visits'\n",
    "chain_ids = ['lowes-home-improvement']\n",
    "descriptions = ''\n",
    "date_range = ['2021-01-24','2022-07-16'] #example['2022-03-20','2022-03-26']\n",
    "timeslice = 'weekly'\n",
    "dow_start = 'Sun'\n",
    "states = ''\n",
    "dmas = ''\n",
    "chain_desc_operator = 'AND'\n",
    "state_dma_operator = 'AND'\n",
    "usage_case = 'General'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(dow_start) > 0):\n",
    "    dow_start = dow_start[0]\n",
    "\n",
    "\n",
    "# Step 2 - Use the description and the chain_ids to filter from placeiq_analytics.category_id_count_per_chain\n",
    "\n",
    "cat_id_table = spark.sql(\"SELECT * FROM placeiq_analytics.category_id_count_per_chain_new\")\n",
    "\n",
    "# if len(chain_ids) > 0:\n",
    "#     cat_id_table = cat_id_table.filter(col('chain_id').isin(chain_ids))\n",
    "\n",
    "# # assuming an \"AND\" between chain_id and description\n",
    "# if len(descriptions) > 0:\n",
    "#     cat_id_table = cat_id_table.filter(col('description').isin(descriptions))\n",
    "\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ((len(chain_ids) > 0) & (len(descriptions) > 0)):\n",
    "    if chain_desc_operator == 'AND':\n",
    "        cat_id_table = cat_id_table.filter((col('chain_id').isin(chain_ids)) & (col('description').isin(descriptions)))\n",
    "    else:\n",
    "        cat_id_table = cat_id_table.filter((col('chain_id').isin(chain_ids)) | (col('description').isin(descriptions)))\n",
    "elif (len(chain_ids)>0):\n",
    "    cat_id_table = cat_id_table.filter(col('chain_id').isin(chain_ids))\n",
    "elif (len(descriptions) > 0):\n",
    "    cat_id_table = cat_id_table.filter(col('description').isin(descriptions))\n",
    "else:\n",
    "    cat_id_table = cat_id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the count column\n",
    "cat_id_table = cat_id_table.select('chain_id','category_id','description')\n",
    "\n",
    "# drop all non-chains\n",
    "cat_id_table = cat_id_table.filter(~(col('chain_id') == 'non_chain'))\n",
    "\n",
    "# Step 3 - Explode place table on category_ids and join on chain_id and category_id from Step 2\n",
    "\n",
    "# read in place table\n",
    "place_df = spark.sql(\"SELECT * FROM placeiq_db_new.place\")\n",
    "\n",
    "place_exploded = place_df.withColumn('split',split(col('category_ids'), ','))\\\n",
    "                   .withColumn('exploded', explode(col('split')))\\\n",
    "                   .drop('split').withColumnRenamed('exploded','category_id')\n",
    "\n",
    "#place_filtered = place_exploded.join(cat_id_table, ['chain_id','category_id'])\n",
    "place_filtered = place_exploded.join(cat_id_table, ['chain_id','category_id'])\n",
    "\n",
    "place_filtered = place_filtered.drop('dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Step 4 - Apply appropriate filters, remove duplicates\n",
    "\n",
    "filter for dma/state\n",
    "\n",
    "For Analytics Use Case (narrow)\n",
    "1. is_high_confidence='true'\n",
    "2. Is_open = 'true'\n",
    "3. Is_multiuse='false'\n",
    "4. movement_source_key = '231'\n",
    "\n",
    "Using Daily_Visit_Count (middle):\n",
    "1. Just join a filtered daily_visit_count (using local_date) to filtered places table \n",
    "on spatial_id instead of starting from visit table.\n",
    "\n",
    "For Measurement/Audience (broad):\n",
    "1. Is_open='true'\n",
    "2. Is_high_confidence='true'\n",
    "\"\"\"\n",
    "\n",
    "# read in visit table with dt between date_range[0] and date_range[1]\n",
    "#date_range[0].replace(\"-\",\"\")\n",
    "\n",
    "\n",
    "\n",
    "# filter place on dmas and state\n",
    "# if len(dmas) > 0:\n",
    "#      place_filtered = place_filtered.filter(col('dma_id').isin(dmas))\n",
    "\n",
    "# # filter place on dmas and state - Assumes AND between dma and state\n",
    "# if len(states) > 0:\n",
    "#      place_filtered = place_filtered.filter(col('state').isin(states))\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ((len(dmas) > 0) & (len(states) > 0)):\n",
    "    if state_dma_operator == 'AND':\n",
    "        place_filtered = place_filtered.filter((col('dma_id').isin(dmas)) & (col('state').isin(states)))\n",
    "    else:\n",
    "        place_filtered = place_filtered.filter((col('dma_id').isin(dmas)) | (col('state').isin(states)))\n",
    "elif (len(dmas)>0):\n",
    "    place_filtered = place_filtered.filter(col('dma_id').isin(dmas))\n",
    "elif (len(states) > 0):\n",
    "    place_filtered = place_filtered.filter(col('state').isin(states))\n",
    "else:\n",
    "    place_filtered = place_filtered\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place_filtered.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location_count = place_filtered.groupBy('chain_id').agg(countDistinct('place_id').alias('num_locations')).sort('chain_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if usage_case == 'Analytics':\n",
    "    visit_df = spark.sql(\"\"\"SELECT * FROM placeiq_db_new.visit WHERE dt\n",
    "     BETWEEN '{}' and '{}'\"\"\".format(date_range[0].replace(\"-\",\"\"), date_range[1].replace(\"-\",\"\")))\n",
    "\n",
    "\n",
    "    # ensure is_open = true, is_high_confidence = true, and is_multiuse = false\n",
    "    place_filtered = place_filtered.filter(\n",
    "                    (col('is_open') == 'true') & (col('is_high_confidence') == 'true') & (col('is_multiuse') == 'false')  \n",
    "                    )\n",
    "\n",
    "    # Step 5 - Join Filtered Visit Table to Filtered Place Table on spatial_id\n",
    "    visit_df = visit_df.filter(col('movement_source_key')==231)\n",
    "\n",
    "    place_visits = place_filtered.join(visit_df, ['spatial_id'])\n",
    "\n",
    "    # Step 6 - Filter by comparing visit attribute score >= 0.5* ceiling score\n",
    "    #          Remove duplicates based on device_key, spatial_id, dt\n",
    "\n",
    "    pv_analytics = place_visits\n",
    "    pv_analytics = pv_analytics.filter(\n",
    "                    (col('visit_attribute_score') >= 0.5*(col('ceiling_score')))\n",
    "                    )\n",
    "\n",
    "    # select the important columns\n",
    "\n",
    "    pv_analytics = pv_analytics.select('device_key','place_id','spatial_id','chain_id','category_id',\n",
    "        'description','latitude','longitude','dma_id','unix_time','dt')\n",
    "\n",
    "    # need to remove duplicates for the same (device_id, place_id, dt)\n",
    "    pv_analytics = pv_analytics.dropDuplicates(['device_key','spatial_id','dt'])\n",
    "\n",
    "    # Step 7 - aggregate at the daily level, leaving dma_id as one of the remaining columns\n",
    "    daily_pv_analytics = pv_analytics.groupBy('chain_id','category_id','description','place_id','latitude','longitude','dma_id','dt') \\\n",
    "                                    .agg(count(lit(1)).alias('visit_count'))\n",
    "\n",
    "    ## Step 8 - join to fact_normalization_by_dma_limited to create a multiplication factor column for\n",
    "    #  each site visit (unless using daily_visit_count)\n",
    "\n",
    "    # read in fact_normalization_by_dma_limited using dt = most recent dt (until we just have most recent date)\n",
    "\n",
    "    dma_normal_factor = spark.sql(\"SELECT * from placeiq_db_new.fact_normalization_by_dma_limited\")\n",
    "\n",
    "    dma_normal_factor = dma_normal_factor.drop('dt')\n",
    "\n",
    "    dma_normal_factor = dma_normal_factor.withColumn('norm_date', regexp_replace('norm_date','-',''))\n",
    "\n",
    "    daily_normalized_analytics = daily_pv_analytics.join(dma_normal_factor, (dma_normal_factor.dma_code ==\n",
    "                daily_pv_analytics.dma_id) & (dma_normal_factor.norm_date == daily_pv_analytics.dt))\n",
    "\n",
    "    daily_normalized_analytics = daily_normalized_analytics.drop('dma_code','norm_date')\n",
    "\n",
    "    ## add in norm_count to get to weighted visit_count\n",
    "\n",
    "    daily_normalized_analytics = daily_normalized_analytics.withColumn(\"visits_norm\", col('norm_factor') * col('visit_count')) \\\n",
    "                                                        .withColumn(\"dt\", to_date(\"dt\",\"yyyyMMdd\"))\n",
    "\n",
    "###########################################################################################################################\n",
    "elif usage_case == 'General':\n",
    "    place_filtered = place_filtered.filter(\n",
    "                    (col('is_open') == 'true') & (col('is_multiuse') == 'false') #& (col('is_high_confidence') == 'true') \n",
    "                    )\n",
    "\n",
    "    visit_df = spark.sql(\"\"\"SELECT * FROM placeiq_db_new.daily_visit_counts WHERE dt\n",
    "                            BETWEEN '{}' and '{}'\"\"\".format(date_range[0].replace(\"-\",\"\"), date_range[1].replace(\"-\",\"\")))\n",
    "\n",
    "    daily_normalized_analytics = place_filtered.join(visit_df, ['spatial_id'])\n",
    "    \n",
    "    daily_normalized_analytics = daily_normalized_analytics.withColumn(\"dt\", to_date(\"dt\",\"yyyyMMdd\"))\n",
    "\n",
    "    # need to remove duplicates for the same (place_id, dt)\n",
    "    daily_normalized_analytics = daily_normalized_analytics.drop_duplicates(['place_id','dt'])\n",
    "\n",
    "elif usage_case =='Targeting':\n",
    "    visit_df = spark.sql(\"\"\"SELECT * FROM placeiq_db_new.visit WHERE dt\n",
    "     BETWEEN '{}' and '{}'\"\"\".format(date_range[0].replace(\"-\",\"\"), date_range[1].replace(\"-\",\"\")))\n",
    "\n",
    "    place_filtered = place_filtered.filter(\n",
    "                    (col('is_open') == 'true') #& (col('is_high_confidence') == 'true') & (col('is_multiuse') == 'false')\n",
    "                    )\n",
    "    ## join to daily visits count\n",
    "    place_visits = place_filtered.join(visit_df, ['spatial_id'])\n",
    "\n",
    "    pv_analytics = place_visits\n",
    "    pv_analytics = pv_analytics.filter(\n",
    "                    (col('visit_attribute_score') >= 0.1*(col('ceiling_score')))\n",
    "                    )\n",
    "\n",
    "    pv_analytics = pv_analytics.select('device_key','place_id','spatial_id','chain_id','category_id',\n",
    "        'description','latitude','longitude','dma_id','unix_time','dt')\n",
    "\n",
    "    # need to remove duplicates for the same (device_id, place_id, dt)\n",
    "    pv_analytics = pv_analytics.dropDuplicates(['device_key','spatial_id','dt'])\n",
    "    \n",
    "    # Step 7 - aggregate at the daily level, leaving dma_id as one of the remaining columns\n",
    "    daily_normalized_analytics = pv_analytics.groupBy('chain_id','category_id','description','place_id','latitude','longitude','dma_id','dt') \\\n",
    "                                    .agg(count(lit(1)).alias('visits_norm')) \n",
    "                                    \n",
    "\n",
    "    daily_normalized_analytics = daily_normalized_analytics.withColumn(\"dt\", to_date(\"dt\",\"yyyyMMdd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#daily_normalized_analytics.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_normalized_analytics.groupBy('chain_id').agg(countDistinct('place_id').alias('num_locations')).sort('chain_id').show()\n",
    "#daily_normalized_analytics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9 - use time slice to group visit counts by appropriate aggregation level as provided by json\n",
    "\n",
    "######### create dict for the different date formats\n",
    "\n",
    "date_format_dict = {\"daily\":\"yyyy-MM-dd\",\n",
    "                    \"monthly\":\"yyyy-MM\",\n",
    "                    \"yearly\":\"yyyy\"}\n",
    "\n",
    "# Spark considers Sunday to be the first day of the week and Saturday to be the last day of the week.\n",
    "# You’ll need to pass in an optional lastDayOfWeek argument if you’d like to use a custom week definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "if timeslice == 'weekly':\n",
    "    timeslice_aggregated_df = daily_normalized_analytics.withColumn('week_end', next_day(col('dt'),dow_start))\\\n",
    "                                                     .withColumn('week_start', date_add(col('week_end'), -6)) \\\n",
    "                                                     .withColumn('timeslice', concat(col('week_start'), lit(' - '), col('week_end')))\n",
    "\n",
    "    timeslice_aggregated_df = timeslice_aggregated_df.groupBy('chain_id','category_id','description','place_id','latitude','longitude','dma_id','timeslice') \\\n",
    "                                                     .agg(round(sum('visits_norm'),0).alias('visit_count'))\n",
    "    \n",
    "elif timeslice == 'quarterly':\n",
    "    timeslice_aggregated_df = daily_normalized_analytics.withColumn('quarter', quarter(col('dt'))) \\\n",
    "                                                        .withColumn('year', date_format(col('dt'), 'yyyy')) \\\n",
    "                                                        .withColumn('timeslice', concat(col('year'), lit('-Q'), col('quarter')))\n",
    "                                                        \n",
    "    timeslice_aggregated_df = timeslice_aggregated_df.groupBy('chain_id','category_id','description','place_id','latitude','longitude','dma_id','timeslice') \\\n",
    "                                                     .agg(round(sum('visits_norm'),0).alias('visit_count'))\n",
    "\n",
    "else:\n",
    "    timeslice_aggregated_df = daily_normalized_analytics.groupBy('chain_id','category_id','description','place_id','latitude','longitude','dma_id',\n",
    "        date_format(\"dt\",date_format_dict[timeslice]). \\\n",
    "        alias('timeslice')).agg(round(sum('visits_norm'),0).alias('visit_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeslice_aggregated_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeslice_aggregated_df.groupBy('chain_id').agg(countDistinct('place_id').alias('num_locations')).sort('chain_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10 - Write to appropriate table. Decide on important columns. Be sure to include run name.\n",
    "\n",
    "\n",
    "table_out = timeslice_aggregated_df\n",
    "\n",
    "table_out = table_out.withColumn('run_name', lit(run_name))\n",
    "\n",
    "table_out = table_out.withColumn('date_created', lit(date_format(current_timestamp(),\"yyyy-MM-dd HH:mm\")))\n",
    "\n",
    "table_out = table_out.withColumn('usage_case', lit(usage_case))\n",
    "\n",
    "#output_table.write.mode(\"overwrite\").saveAsTable(\"placeiq_analytics.testing_hive_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table_out.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table_out.groupBy('chain_id').agg(countDistinct('place_id').alias('num_locations')).sort('chain_id').show()\n",
    "\n",
    "if len(table_out.head(1)) == 0:\n",
    "    print(\"no rows found\")\n",
    "    row_warning = [(\"no rows returned\",\"check run config\",\"check to make sure\",\"there are stores\",\"with\" ,\"If usage_case=Analytics is_open=true is_high_confidence=true and is_multiuse=false\",\"if general check is_open=true is_multiuse=false\",\"if targeting check is_open=true\",0,run_name,'',usage_case)]\n",
    "    col_names = [\"chain_id\",\"category_id\",\"description\",\"place_id\",\"latitude\",\"longitude\",\"dma_id\",\"timeslice\",\"visit_count\",\"run_name\",\"date_created\",\"usage_case\"]\n",
    "    table_out = spark.createDataFrame(data=row_warning,schema=col_names)\n",
    "\n",
    "    table_out = table_out.withColumn('date_created',lit(date_format(current_timestamp(),\"yyyy-MM-dd HH:mm\")))\n",
    "    table_out = table_out.select(\"chain_id\",\"category_id\",\"description\",\"place_id\",\"latitude\",\"longitude\",\"dma_id\",\"timeslice\",\"visit_count\",\"run_name\",\"date_created\",\"usage_case\")\n",
    "    table_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#table_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table_out.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.place_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"aldiStores_testRun.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
